{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e21b9a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\02Office_software\\anaconda3\\envs\\Torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from collections import namedtuple\n",
    "import itertools\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import sys\n",
    "import scipy.stats\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from simulation_env_new import Env\n",
    "from tqdm import tqdm\n",
    "from FourWI_EV_new import EV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bc168e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, action_bound):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc_mu = torch.nn.Linear(hidden_dim, action_dim)\n",
    "        self.fc_std = torch.nn.Linear(hidden_dim, action_dim)\n",
    "        self.action_bound = action_bound\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mu = F.tanh(self.fc_mu(x))\n",
    "        std = F.softplus(self.fc_std(x)) + 0.01\n",
    "        dist = Normal(mu, std)\n",
    "        normal_sample = dist.rsample()  # rsample()是重参数化采样函数    \n",
    "        log_prob = dist.log_prob(normal_sample)\n",
    "        action = torch.tanh(normal_sample)  # 计算tanh_normal分布的对数概率密度\n",
    "        log_prob = log_prob - torch.log(1 - torch.tanh(action).pow(2) + 1e-7)\n",
    "        action = action * self.action_bound\n",
    "        return action, log_prob\n",
    "\n",
    "class QValueNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(QValueNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        cat = torch.cat([x, a], dim=1)  # 拼接状态和动作\n",
    "        x = F.relu(self.fc1(cat))\n",
    "        return self.fc2(x)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
    "    \"cpu\")\n",
    "\n",
    "class SAC:\n",
    "    ''' 处理连续动作的SAC算法 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, action_bound,\n",
    "                 actor_lr, critic_lr, alpha_lr, target_entropy, tau, gamma):\n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim,\n",
    "                               action_bound).to(device)  # 策略网络\n",
    "        # 第一个Q网络\n",
    "        self.critic_1 = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        # 第二个Q网络\n",
    "        self.critic_2 = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.target_critic_1 = QValueNet(state_dim, hidden_dim,\n",
    "                                         action_dim).to(device)  # 第一个目标Q网络\n",
    "        self.target_critic_2 = QValueNet(state_dim, hidden_dim,\n",
    "                                         action_dim).to(device)  # 第二个目标Q网络\n",
    "        # 令目标Q网络的初始参数和Q网络一样\n",
    "        self.target_critic_1.load_state_dict(self.critic_1.state_dict())\n",
    "        self.target_critic_2.load_state_dict(self.critic_2.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),\n",
    "                                                lr=actor_lr)\n",
    "        self.critic_1_optimizer = torch.optim.Adam(self.critic_1.parameters(),\n",
    "                                                   lr=critic_lr)\n",
    "        self.critic_2_optimizer = torch.optim.Adam(self.critic_2.parameters(),\n",
    "                                                   lr=critic_lr)\n",
    "        # 使用alpha的log值,可以使训练结果比较稳定\n",
    "        self.log_alpha = torch.tensor(np.log(0.01), dtype=torch.float)\n",
    "        self.log_alpha.requires_grad = True  # 可以对alpha求梯度\n",
    "        self.log_alpha_optimizer = torch.optim.Adam([self.log_alpha],\n",
    "                                                    lr=alpha_lr)\n",
    "        self.target_entropy = target_entropy  # 目标熵的大小\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor([state], dtype=torch.float).to(device)\n",
    "        action = self.actor(state)[0]\n",
    "        return [action.item()]\n",
    "\n",
    "    def calc_target(self, rewards, next_states, dones):  # 计算目标Q值\n",
    "        next_actions, log_prob = self.actor(next_states)\n",
    "        entropy = -log_prob\n",
    "        q1_value = self.target_critic_1(next_states, next_actions)\n",
    "        q2_value = self.target_critic_2(next_states, next_actions)\n",
    "        next_value = torch.min(q1_value,\n",
    "                               q2_value) + self.log_alpha.exp() * entropy\n",
    "        td_target = rewards + self.gamma * next_value * (1 - dones)\n",
    "        return td_target\n",
    "\n",
    "    def soft_update(self, net, target_net):\n",
    "        for param_target, param in zip(target_net.parameters(),\n",
    "                                       net.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - self.tau) +\n",
    "                                    param.data * self.tau)\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'],\n",
    "                              dtype=torch.float).to(device)\n",
    "        actions = torch.tensor(transition_dict['actions'],\n",
    "                               dtype=torch.float).view(-1, 1).to(device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'],\n",
    "                               dtype=torch.float).view(-1, 1).to(device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'],\n",
    "                                   dtype=torch.float).to(device)\n",
    "        dones = torch.tensor(transition_dict['dones'],\n",
    "                             dtype=torch.float).view(-1, 1).to(device)\n",
    "        rewards = (rewards + 1) / 1  # 对倒立摆环境的奖励进行重塑\n",
    "\n",
    "        # 更新两个Q网络\n",
    "        td_target = self.calc_target(rewards, next_states, dones)\n",
    "        critic_1_loss = torch.mean(\n",
    "            F.mse_loss(self.critic_1(states, actions), td_target.detach()))\n",
    "        critic_2_loss = torch.mean(\n",
    "            F.mse_loss(self.critic_2(states, actions), td_target.detach()))\n",
    "        self.critic_1_optimizer.zero_grad()\n",
    "        critic_1_loss.backward()\n",
    "        self.critic_1_optimizer.step()\n",
    "        self.critic_2_optimizer.zero_grad()\n",
    "        critic_2_loss.backward()\n",
    "        self.critic_2_optimizer.step()\n",
    "\n",
    "        # 更新策略网络\n",
    "        new_actions, log_prob = self.actor(states)\n",
    "        entropy = -log_prob\n",
    "        q1_value = self.critic_1(states, new_actions)\n",
    "        q2_value = self.critic_2(states, new_actions)\n",
    "        actor_loss = torch.mean(-self.log_alpha.exp() * entropy -\n",
    "                                torch.min(q1_value, q2_value))\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # 更新alpha值\n",
    "        alpha_loss = torch.mean(\n",
    "            (entropy - self.target_entropy).detach() * self.log_alpha.exp())\n",
    "        self.log_alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.log_alpha_optimizer.step()\n",
    "\n",
    "        self.soft_update(self.critic_1, self.target_critic_1)\n",
    "        self.soft_update(self.critic_2, self.target_critic_2)\n",
    "        \n",
    "    def save(self,episode):\n",
    "        torch.save(self.actor.state_dict(), \"./model/MBPO/mbpo_s3_actor{}.pth\".format(episode))\n",
    "\n",
    "    def load(self,episode):\n",
    "        self.actor.load_state_dict(torch.load(\"./model/sac_actor{}.pth\".format(episode)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddccb798",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    ''' Swish激活函数 '''\n",
    "    def __init__(self):\n",
    "        super(Swish, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    ''' 初始化模型权重 '''\n",
    "    def truncated_normal_init(t, mean=0.0, std=0.01):\n",
    "        torch.nn.init.normal_(t, mean=mean, std=std)\n",
    "        while True:\n",
    "            cond = (t < mean - 2 * std) | (t > mean + 2 * std)\n",
    "            if not torch.sum(cond):\n",
    "                break\n",
    "            t = torch.where(\n",
    "                cond,\n",
    "                torch.nn.init.normal_(torch.ones(t.shape, device=device),\n",
    "                                      mean=mean,\n",
    "                                      std=std), t)\n",
    "        return t\n",
    "\n",
    "    if type(m) == nn.Linear or isinstance(m, FCLayer):\n",
    "        truncated_normal_init(m.weight, std=1 / (2 * np.sqrt(m._input_dim)))\n",
    "        m.bias.data.fill_(0.0)\n",
    "\n",
    "\n",
    "class FCLayer(nn.Module):\n",
    "    ''' 集成之后的全连接层 '''\n",
    "    def __init__(self, input_dim, output_dim, ensemble_size, activation):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self._input_dim, self._output_dim = input_dim, output_dim\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.Tensor(ensemble_size, input_dim, output_dim).to(device))\n",
    "        self._activation = activation\n",
    "        self.bias = nn.Parameter(\n",
    "            torch.Tensor(ensemble_size, output_dim).to(device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._activation(\n",
    "            torch.add(torch.bmm(x, self.weight), self.bias[:, None, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05d4fb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel(nn.Module):\n",
    "    ''' 环境模型集成 '''\n",
    "    def __init__(self,\n",
    "                 state_dim,\n",
    "                 action_dim,\n",
    "                 model_alpha,\n",
    "                 ensemble_size=5,\n",
    "                 learning_rate=1e-4):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        # 输出包括均值和方差,因此是状态与奖励维度之和的两倍\n",
    "        self._output_dim = (state_dim + 1) * 2\n",
    "        self._model_alpha = model_alpha  # 模型损失函数中加权时的权重\n",
    "        self._max_logvar = nn.Parameter((torch.ones(\n",
    "            (1, self._output_dim // 2)).float() / 2).to(device),\n",
    "                                        requires_grad=False)\n",
    "        self._min_logvar = nn.Parameter((-torch.ones(\n",
    "            (1, self._output_dim // 2)).float() * 10).to(device),\n",
    "                                        requires_grad=False)\n",
    "\n",
    "        self.layer1 = FCLayer(state_dim + action_dim, 200, ensemble_size,\n",
    "                              Swish())\n",
    "        self.layer2 = FCLayer(200, 200, ensemble_size, Swish())\n",
    "        self.layer3 = FCLayer(200, 200, ensemble_size, Swish())\n",
    "        self.layer4 = FCLayer(200, 200, ensemble_size, Swish())\n",
    "        self.layer5 = FCLayer(200, self._output_dim, ensemble_size,\n",
    "                              nn.Identity())\n",
    "        self.apply(init_weights)  # 初始化环境模型中的参数\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, x, return_log_var=False):\n",
    "        ret = self.layer5(self.layer4(self.layer3(self.layer2(\n",
    "            self.layer1(x)))))\n",
    "        mean = ret[:, :, :self._output_dim // 2]\n",
    "        # 在PETS算法中,将方差控制在最小值和最大值之间\n",
    "        logvar = self._max_logvar - F.softplus(\n",
    "            self._max_logvar - ret[:, :, self._output_dim // 2:])\n",
    "        logvar = self._min_logvar + F.softplus(logvar - self._min_logvar)\n",
    "        return mean, logvar if return_log_var else torch.exp(logvar)\n",
    "\n",
    "    def loss(self, mean, logvar, labels, use_var_loss=True):\n",
    "        inverse_var = torch.exp(-logvar)\n",
    "        if use_var_loss:\n",
    "            mse_loss = torch.mean(torch.mean(torch.pow(mean - labels, 2) *\n",
    "                                             inverse_var,\n",
    "                                             dim=-1),\n",
    "                                  dim=-1)\n",
    "            var_loss = torch.mean(torch.mean(logvar, dim=-1), dim=-1)\n",
    "            total_loss = torch.sum(mse_loss) + torch.sum(var_loss)\n",
    "        else:\n",
    "            mse_loss = torch.mean(torch.pow(mean - labels, 2), dim=(1, 2))\n",
    "            total_loss = torch.sum(mse_loss)\n",
    "        return total_loss, mse_loss\n",
    "\n",
    "    def train(self, loss):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss += self._model_alpha * torch.sum(\n",
    "            self._max_logvar) - self._model_alpha * torch.sum(self._min_logvar)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "class EnsembleDynamicsModel:\n",
    "    ''' 环境模型集成,加入精细化的训练 '''\n",
    "    def __init__(self, state_dim, action_dim, model_alpha=0.01, num_network=5):\n",
    "        self._num_network = num_network\n",
    "        self._state_dim, self._action_dim = state_dim, action_dim\n",
    "        self.model = EnsembleModel(state_dim,\n",
    "                                   action_dim,\n",
    "                                   model_alpha,\n",
    "                                   ensemble_size=num_network)\n",
    "        self._epoch_since_last_update = 0\n",
    "\n",
    "    def train(self,\n",
    "              inputs,\n",
    "              labels,\n",
    "              batch_size=256,\n",
    "              holdout_ratio=0.1,\n",
    "              max_iter=20):\n",
    "        # 设置训练集与验证集\n",
    "        permutation = np.random.permutation(inputs.shape[0])\n",
    "        inputs, labels = inputs[permutation], labels[permutation]\n",
    "        num_holdout = int(inputs.shape[0] * holdout_ratio)\n",
    "        train_inputs, train_labels = inputs[num_holdout:], labels[num_holdout:]\n",
    "        holdout_inputs, holdout_labels = inputs[:\n",
    "                                                num_holdout], labels[:\n",
    "                                                                     num_holdout]\n",
    "        holdout_inputs = torch.from_numpy(holdout_inputs).float().to(device)\n",
    "        holdout_labels = torch.from_numpy(holdout_labels).float().to(device)\n",
    "        holdout_inputs = holdout_inputs[None, :, :].repeat(\n",
    "            [self._num_network, 1, 1])\n",
    "        holdout_labels = holdout_labels[None, :, :].repeat(\n",
    "            [self._num_network, 1, 1])\n",
    "\n",
    "        # 保留最好的结果\n",
    "        self._snapshots = {i: (None, 1e10) for i in range(self._num_network)}\n",
    "\n",
    "        for epoch in itertools.count():\n",
    "            # 定义每一个网络的训练数据\n",
    "            train_index = np.vstack([\n",
    "                np.random.permutation(train_inputs.shape[0])\n",
    "                for _ in range(self._num_network)\n",
    "            ])\n",
    "            # 所有真实数据都用来训练\n",
    "            for batch_start_pos in range(0, train_inputs.shape[0], batch_size):\n",
    "                batch_index = train_index[:, batch_start_pos:batch_start_pos +\n",
    "                                          batch_size]\n",
    "                train_input = torch.from_numpy(\n",
    "                    train_inputs[batch_index]).float().to(device)\n",
    "                train_label = torch.from_numpy(\n",
    "                    train_labels[batch_index]).float().to(device)\n",
    "\n",
    "                mean, logvar = self.model(train_input, return_log_var=True)\n",
    "                loss, _ = self.model.loss(mean, logvar, train_label)\n",
    "                self.model.train(loss)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                mean, logvar = self.model(holdout_inputs, return_log_var=True)\n",
    "                _, holdout_losses = self.model.loss(mean,\n",
    "                                                    logvar,\n",
    "                                                    holdout_labels,\n",
    "                                                    use_var_loss=False)\n",
    "                holdout_losses = holdout_losses.cpu()\n",
    "                break_condition = self._save_best(epoch, holdout_losses)\n",
    "                if break_condition or epoch > max_iter:  # 结束训练\n",
    "                    break\n",
    "        return loss\n",
    "\n",
    "    def _save_best(self, epoch, losses, threshold=0.1):\n",
    "        updated = False\n",
    "        for i in range(len(losses)):\n",
    "            current = losses[i]\n",
    "            _, best = self._snapshots[i]\n",
    "            improvement = (best - current) / best\n",
    "            if improvement > threshold:\n",
    "                self._snapshots[i] = (epoch, current)\n",
    "                updated = True\n",
    "        self._epoch_since_last_update = 0 if updated else self._epoch_since_last_update + 1\n",
    "        return self._epoch_since_last_update > 5\n",
    "\n",
    "    def predict(self, inputs, batch_size=64):\n",
    "        inputs = np.tile(inputs, (self._num_network, 1, 1))\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float).to(device)\n",
    "        mean, var = self.model(inputs, return_log_var=False)\n",
    "        return mean.detach().cpu().numpy(), var.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "class FakeEnv:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def step(self, obs, act):\n",
    "        inputs = np.concatenate((obs, act), axis=-1)\n",
    "        ensemble_model_means, ensemble_model_vars = self.model.predict(inputs)\n",
    "        ensemble_model_means[:, :, 1:] += obs\n",
    "        ensemble_model_stds = np.sqrt(ensemble_model_vars)\n",
    "        ensemble_samples = ensemble_model_means + np.random.normal(\n",
    "            size=ensemble_model_means.shape) * ensemble_model_stds\n",
    "\n",
    "        num_models, batch_size, _ = ensemble_model_means.shape\n",
    "        models_to_use = np.random.choice(\n",
    "            [i for i in range(self.model._num_network)], size=batch_size)\n",
    "        batch_inds = np.arange(0, batch_size)\n",
    "        samples = ensemble_samples[models_to_use, batch_inds]\n",
    "        rewards, next_obs = samples[:, :1][0][0], samples[:, 1:][0]\n",
    "        return rewards, next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32dcf6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class variableK:\n",
    "    def __init__(self,env_pool, model_pool):\n",
    "        self.env_pool = env_pool\n",
    "        self.model_pool = model_pool\n",
    "\n",
    "    def KLdivergence(self, batch_size=100, epsilon = 1e-6):\n",
    "        env_obs, env_action, env_reward, env_next_obs, env_done = self.env_pool.sample(batch_size) \n",
    "        model_obs, model_action, model_reward, model_next_obs, model_done = self.model_pool.sample(batch_size)\n",
    "        KL_obs = scipy.stats.entropy(abs(env_next_obs) + epsilon, abs(model_next_obs) + epsilon)\n",
    "        KL_reward = scipy.stats.entropy((np.array(env_reward)+3)/3 + epsilon, (np.array(model_reward)+3)/3 + epsilon)\n",
    "        return KL_obs, KL_reward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "488305a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBPO:\n",
    "    def __init__(self, env, agent, fake_env, env_pool, model_pool,rollout_K, rollout_batch_size, real_ratio, num_episode):\n",
    "\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.fake_env = fake_env\n",
    "        self.env_pool = env_pool\n",
    "        self.model_pool = model_pool\n",
    "        self.rollout_batch_size = rollout_batch_size\n",
    "        self.real_ratio = real_ratio\n",
    "        self.num_episode = num_episode\n",
    "        self.rollout_K = rollout_K\n",
    "\n",
    "    def rollout_model(self):\n",
    "        observations, _, _, _, _ = self.env_pool.sample(\n",
    "            self.rollout_batch_size)\n",
    "        if self.model_pool.size() > 100:\n",
    "            KL_obs, KL_reward = self.rollout_K.KLdivergence()\n",
    "            K = np.clip(round(2.5/ max(KL_obs[0],KL_obs[1],KL_obs[2],KL_reward) ), 0, 5)\n",
    "        else:\n",
    "            K = 1\n",
    "  \n",
    "        for obs in observations:\n",
    "            for i in range(K):\n",
    "                action = self.agent.take_action(obs)\n",
    "                reward, next_obs = self.fake_env.step(obs, action)\n",
    "                self.model_pool.add(obs, action, reward, next_obs, False)\n",
    "                obs = next_obs\n",
    "        return K\n",
    "\n",
    "    def update_agent(self, policy_train_batch_size=64):\n",
    "        env_batch_size = int(policy_train_batch_size * self.real_ratio)\n",
    "        model_batch_size = policy_train_batch_size - env_batch_size\n",
    "        for epoch in range(10):\n",
    "            env_obs, env_action, env_reward, env_next_obs, env_done = self.env_pool.sample(\n",
    "                env_batch_size)\n",
    "            if self.model_pool.size() > 0:\n",
    "                model_obs, model_action, model_reward, model_next_obs, model_done = self.model_pool.sample(\n",
    "                    model_batch_size)\n",
    "                obs = np.concatenate((env_obs, model_obs), axis=0)\n",
    "                env_action = env_action[:,np.newaxis]\n",
    "                action = np.concatenate((env_action, model_action), axis=0)\n",
    "                next_obs = np.concatenate((env_next_obs, model_next_obs),\n",
    "                                          axis=0)\n",
    "                reward = np.concatenate((env_reward, model_reward), axis=0)\n",
    "                done = np.concatenate((env_done, model_done), axis=0)\n",
    "            else:\n",
    "                obs, action, next_obs, reward, done = env_obs, env_action, env_next_obs, env_reward, env_done\n",
    "            transition_dict = {\n",
    "                'states': obs,\n",
    "                'actions': action,\n",
    "                'next_states': next_obs,\n",
    "                'rewards': reward,\n",
    "                'dones': done\n",
    "            }\n",
    "            self.agent.update(transition_dict)\n",
    "            \n",
    "\n",
    "    def train_model(self):\n",
    "        obs, action, reward, next_obs, done = self.env_pool.return_all_samples()\n",
    "        action = action[:,np.newaxis]\n",
    "        inputs = np.concatenate((obs, action), axis=-1)\n",
    "        reward = np.array(reward)\n",
    "        labels = np.concatenate(\n",
    "            (np.reshape(reward, (reward.shape[0], -1)), next_obs - obs),\n",
    "            axis=-1)\n",
    "        loss = self.fake_env.model.train(inputs, labels)\n",
    "        return loss\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if batch_size > len(self.buffer):\n",
    "            return self.return_all_samples()\n",
    "        else:\n",
    "            transitions = random.sample(self.buffer, batch_size)\n",
    "            state, action, reward, next_state, done = zip(*transitions)\n",
    "            return np.array(state), np.array(action), reward, np.array(next_state), done\n",
    "\n",
    "    def return_all_samples(self):\n",
    "        all_transitions = list(self.buffer)\n",
    "        state, action, reward, next_state, done = zip(*all_transitions)\n",
    "        return np.array(state), np.array(action), reward, np.array(next_state), done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1539b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "CA = False\n",
    "total_episode = 200\n",
    "TTC_threshold = 3.001\n",
    "base_name = f'MBPO_VarK_Energy_s3' \n",
    "\n",
    "real_ratio = 0.8\n",
    "actor_lr = 0.001\n",
    "critic_lr = 0.001\n",
    "alpha_lr = 3e-4\n",
    "hidden_dim = 32\n",
    "gamma = 0.9\n",
    "tau = 0.005  # 软更新参数\n",
    "MEMORY_CAPACITY = 20000\n",
    "batch_size = 1024\n",
    "target_entropy = -1\n",
    "model_alpha = 0.01  # 模型损失函数中的加权权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fe42362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 1073\n",
      "Number of validate samples: 268\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "env = Env(TTC_threshold)\n",
    "\n",
    "# load training data\n",
    "# train = sio.loadmat('trainSet.mat')['calibrationData']\n",
    "# test = sio.loadmat('testSet.mat')['validationData']\n",
    "train = sio.loadmat('calibrationData_new.mat')['calibrationData_new']\n",
    "test = sio.loadmat('validationData_new.mat')['validationData_new']\n",
    "trainNum = train.shape[0]\n",
    "testNum = test.shape[0]\n",
    "print('Number of training samples:', trainNum)\n",
    "print('Number of validate samples:', testNum)\n",
    "\n",
    "random.seed(3)\n",
    "np.random.seed(3)\n",
    "torch.manual_seed(3)\n",
    "\n",
    "target_entropy = -env.n_actions\n",
    "s_dim = env.n_features\n",
    "a_dim = env.n_actions\n",
    "a_bound = env.action_Bound\n",
    "\n",
    "rollout_batch_size = 100\n",
    "rollout_length = 5  # 推演长度k,推荐更多尝试\n",
    "model_pool_size = rollout_batch_size * rollout_length\n",
    "\n",
    "n_run = 3\n",
    "rolling_window = 10  # 100 car following events, average score\n",
    "result = []\n",
    "Ev = EV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f7f485",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10678\\AppData\\Local\\Temp\\ipykernel_139652\\452914256.py:15: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_rolling_score = np.float('-inf')\n",
      "C:\\Users\\10678\\AppData\\Local\\Temp\\ipykernel_139652\\452914256.py:16: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_score = np.float('-inf')\n",
      "C:\\Users\\10678\\AppData\\Local\\Temp\\ipykernel_139652\\126516055.py:68: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  state = torch.tensor([state], dtype=torch.float).to(device)\n",
      "D:\\02Office_software\\anaconda3\\envs\\Torch\\lib\\site-packages\\torch\\nn\\functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout_length =  1\n",
      "rollout_length =  1\n",
      "KL_obs =  [0.58681862 0.02504021 0.5773291 ]\n",
      "KL_reward =  0.01970073635733089\n",
      "rollout_length =  4\n",
      "KL_obs =  [0.55035491 0.02215195 0.52674147]\n",
      "KL_reward =  0.01132509053454503\n",
      "rollout_length =  5\n",
      "KL_obs =  [0.49539817 0.02443335 0.55782507]\n",
      "KL_reward =  0.01478904062178926\n",
      "rollout_length =  4\n",
      "KL_obs =  [0.47249402 0.03265186 0.61688119]\n",
      "KL_reward =  0.011616814261570211\n",
      "rollout_length =  4\n",
      "KL_obs =  [0.38460578 0.0290349  0.44609332]\n",
      "KL_reward =  0.011463287989510022\n",
      "rollout_length =  5\n",
      "KL_obs =  [0.35782233 0.0296381  0.55326049]\n",
      "KL_reward =  0.010542309019758599\n",
      "rollout_length =  5\n",
      "KL_obs =  [0.27605276 0.03321809 0.87998401]\n",
      "KL_reward =  0.00963067933304209\n",
      "rollout_length =  3\n",
      "KL_obs =  [0.50381067 0.03104865 0.71854988]\n",
      "KL_reward =  0.0141887632241195\n",
      "rollout_length =  3\n",
      "KL_obs =  [0.66775817 0.03854353 0.90435841]\n",
      "KL_reward =  0.016132433851149937\n",
      "rollout_length =  3\n",
      "KL_obs =  [0.65889665 0.05196795 0.68470233]\n",
      "KL_reward =  0.012415164326972353\n",
      "rollout_length =  4\n",
      "KL_obs =  [1.11417758 0.06174545 0.65405843]\n",
      "KL_reward =  0.012928324144648517\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.84921941 0.04076303 1.00039125]\n",
      "KL_reward =  0.019580685442409897\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.99614057 0.05071438 0.73107181]\n",
      "KL_reward =  0.011667220704001068\n",
      "rollout_length =  3\n",
      " Run MBPO_VarK_Energy_s3, Episode 0, Score: 0.07, Rolling score: 0.07, loss: -15.87, Max rolling score: 0.07, collisions: 0   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/200 [00:34<1:53:45, 34.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL_obs =  [0.98405255 0.04208452 0.72957973]\n",
      "KL_reward =  0.0168762458583127\n",
      "rollout_length =  3\n",
      "KL_obs =  [1.02060211 0.03717879 0.67751131]\n",
      "KL_reward =  0.01743794142418442\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.99213364 0.0476702  1.02025568]\n",
      "KL_reward =  0.017497621119096356\n",
      "rollout_length =  2\n",
      "KL_obs =  [1.12534165 0.05165295 0.92233434]\n",
      "KL_reward =  0.015296125939404668\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.6977383  0.04602783 0.61495233]\n",
      "KL_reward =  0.016755690132341808\n",
      "rollout_length =  4\n",
      "KL_obs =  [1.17410576 0.05280331 0.77346104]\n",
      "KL_reward =  0.013881892027296927\n",
      "rollout_length =  2\n",
      "KL_obs =  [1.09322237 0.0490224  0.90346081]\n",
      "KL_reward =  0.01709206863580337\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.81112803 0.04162937 0.90189095]\n",
      "KL_reward =  0.0174644065239225\n",
      "rollout_length =  3\n",
      "KL_obs =  [0.95723008 0.0462748  1.05797319]\n",
      "KL_reward =  0.014715013938196844\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.83848393 0.04212767 0.96401854]\n",
      "KL_reward =  0.012793243670941371\n",
      "rollout_length =  3\n",
      "KL_obs =  [1.04265453 0.04566845 0.82083428]\n",
      "KL_reward =  0.01657440855739481\n",
      "rollout_length =  2\n",
      " Run MBPO_VarK_Energy_s3, Episode 1, Score: 0.52, Rolling score: 0.29, loss: -22.62, Max rolling score: 0.29, collisions: 0   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 2/200 [00:56<1:30:22, 27.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL_obs =  [1.00947599 0.0438737  0.84223216]\n",
      "KL_reward =  0.015978650892441562\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.88298114 0.04712305 1.20166325]\n",
      "KL_reward =  0.014537220380026455\n",
      "rollout_length =  2\n",
      "KL_obs =  [1.1250732  0.03535629 1.07559251]\n",
      "KL_reward =  0.014896252569183557\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.97468861 0.03553974 1.14877175]\n",
      "KL_reward =  0.016690220939080153\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.90789169 0.04097191 1.28209019]\n",
      "KL_reward =  0.01326738197698474\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.95827027 0.04278179 1.34372631]\n",
      "KL_reward =  0.01962334845176841\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.91974747 0.03395227 1.14623868]\n",
      "KL_reward =  0.011260606418690046\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.74262254 0.03840177 1.02273492]\n",
      "KL_reward =  0.01376876462529977\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.75478975 0.03610871 1.14605218]\n",
      "KL_reward =  0.0117292880492935\n",
      "rollout_length =  2\n",
      " Run MBPO_VarK_Energy_s3, Episode 2, Score: 0.66, Rolling score: 0.42, loss: -26.28, Max rolling score: 0.42, collisions: 0   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 3/200 [01:15<1:16:55, 23.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL_obs =  [0.65849319 0.04432892 0.93880604]\n",
      "KL_reward =  0.01282092186853956\n",
      "rollout_length =  3\n",
      "KL_obs =  [0.75756284 0.04679615 1.3923751 ]\n",
      "KL_reward =  0.014068628668669064\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.96737047 0.04178633 1.03156325]\n",
      "KL_reward =  0.015284423348658881\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.89662535 0.04407797 1.62039136]\n",
      "KL_reward =  0.01625614030875735\n",
      "rollout_length =  2\n",
      "KL_obs =  [1.08948173 0.03804774 0.85263817]\n",
      "KL_reward =  0.018270885346384234\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.78543878 0.04248486 0.91898532]\n",
      "KL_reward =  0.012972137949547788\n",
      "rollout_length =  3\n",
      "KL_obs =  [0.6132616  0.03384596 0.94120754]\n",
      "KL_reward =  0.011442076744001741\n",
      "rollout_length =  3\n",
      "KL_obs =  [0.63445015 0.03552499 1.44980072]\n",
      "KL_reward =  0.010337575296504696\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.71492917 0.03548438 1.52562621]\n",
      "KL_reward =  0.013724212748234417\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.90050083 0.03634218 1.47548629]\n",
      "KL_reward =  0.01773223700039844\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.92035781 0.04295492 1.38584451]\n",
      "KL_reward =  0.012501800496808161\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.73457889 0.03163083 1.52568464]\n",
      "KL_reward =  0.014673004904459497\n",
      "rollout_length =  2\n",
      " Run MBPO_VarK_Energy_s3, Episode 3, Score: 0.69, Rolling score: 0.48, loss: -29.44, Max rolling score: 0.48, collisions: 0   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 4/200 [01:42<1:21:20, 24.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL_obs =  [0.85433451 0.03772742 1.34240516]\n",
      "KL_reward =  0.017532560022000078\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.53403249 0.03480015 1.03948487]\n",
      "KL_reward =  0.010507846831246588\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.74795063 0.04222816 1.93283735]\n",
      "KL_reward =  0.014117286145548288\n",
      "rollout_length =  1\n",
      "KL_obs =  [0.68210744 0.03594713 1.4032465 ]\n",
      "KL_reward =  0.010719579064773548\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.51506319 0.03195116 1.1350857 ]\n",
      "KL_reward =  0.012670010158796168\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.95765011 0.03442498 1.50547   ]\n",
      "KL_reward =  0.018724308691844348\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.62171348 0.03916013 1.20384521]\n",
      "KL_reward =  0.012017898196420244\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.73052389 0.03743156 1.53025288]\n",
      "KL_reward =  0.01299245049606719\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.78458849 0.03727534 1.83737363]\n",
      "KL_reward =  0.014139011655894299\n",
      "rollout_length =  1\n",
      "KL_obs =  [0.75396486 0.0324425  1.36995986]\n",
      "KL_reward =  0.01309486465357456\n",
      "rollout_length =  2\n",
      "KL_obs =  [1.0091999  0.03233695 1.48994107]\n",
      "KL_reward =  0.017586204948764724\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.86842459 0.02778943 1.36545464]\n",
      "KL_reward =  0.01467596436916124\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.75733457 0.03446849 2.02952951]\n",
      "KL_reward =  0.011899685657650216\n",
      "rollout_length =  1\n",
      "KL_obs =  [0.78216154 0.02820678 1.48394635]\n",
      "KL_reward =  0.015538985927751691\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.54147594 0.03367651 1.55605902]\n",
      "KL_reward =  0.012657499860155479\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.9402829 0.0295457 1.4524269]\n",
      "KL_reward =  0.012235508312889535\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.61222532 0.02608276 1.6216557 ]\n",
      "KL_reward =  0.013397339912687815\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.67189153 0.02381848 1.15547318]\n",
      "KL_reward =  0.009780457742714563\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.71796592 0.03167567 1.34623557]\n",
      "KL_reward =  0.014618999509501742\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.76628098 0.02520355 1.59621919]\n",
      "KL_reward =  0.014846537826177127\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.78776041 0.02766756 1.31447848]\n",
      "KL_reward =  0.014095879559409172\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.84064604 0.02600966 1.91107979]\n",
      "KL_reward =  0.016740712248520848\n",
      "rollout_length =  1\n",
      "KL_obs =  [0.74730798 0.03586942 1.53148015]\n",
      "KL_reward =  0.01396600153639005\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.6212014  0.02738059 1.44656442]\n",
      "KL_reward =  0.011895948222412488\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.66785845 0.02418884 1.31141364]\n",
      "KL_reward =  0.0107952491643782\n",
      "rollout_length =  2\n",
      " Run MBPO_VarK_Energy_s3, Episode 4, Score: 0.68, Rolling score: 0.52, loss: -31.89, Max rolling score: 0.52, collisions: 0   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▎         | 5/200 [02:16<1:31:05, 28.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL_obs =  [0.39781595 0.03438127 1.4376697 ]\n",
      "KL_reward =  0.0077167279967779315\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.6523296  0.03524223 1.44836988]\n",
      "KL_reward =  0.008357203329799102\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.46895127 0.02781543 1.79506   ]\n",
      "KL_reward =  0.010201678994622761\n",
      "rollout_length =  1\n",
      "KL_obs =  [0.64722072 0.02237877 1.72024843]\n",
      "KL_reward =  0.012447059785862859\n",
      "rollout_length =  1\n",
      "KL_obs =  [0.46658461 0.03901114 1.54931086]\n",
      "KL_reward =  0.011559873095710828\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.49004389 0.03246124 1.65966701]\n",
      "KL_reward =  0.008576958298683313\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.56976302 0.03399233 1.84610572]\n",
      "KL_reward =  0.011278365481826041\n",
      "rollout_length =  1\n",
      "KL_obs =  [0.54826911 0.03779974 1.17674009]\n",
      "KL_reward =  0.008372687871229648\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.58254902 0.03645715 1.52261784]\n",
      "KL_reward =  0.01501646358594223\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.63771397 0.04304373 1.55065261]\n",
      "KL_reward =  0.00961204444967621\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.46543306 0.03720221 1.37665592]\n",
      "KL_reward =  0.010750535928935776\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.64951826 0.04874861 1.66604832]\n",
      "KL_reward =  0.009992814461940738\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.50174129 0.03974275 1.5986578 ]\n",
      "KL_reward =  0.010056794515466448\n",
      "rollout_length =  2\n",
      "KL_obs =  [0.51083466 0.0420953  1.9294049 ]\n",
      "KL_reward =  0.013134761847853772\n",
      "rollout_length =  1\n",
      " Run MBPO_VarK_Energy_s3, Episode 5, Score: 0.70, Rolling score: 0.55, loss: -32.24, Max rolling score: 0.55, collisions: 0   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 6/200 [02:35<1:20:28, 24.89s/it]"
     ]
    }
   ],
   "source": [
    "for run in [base_name]:\n",
    "    # name is the name of the experiment, CA is whether use collision avoidance\n",
    "    pointer = 0\n",
    "    sac = SAC(s_dim, hidden_dim, a_dim, a_bound, actor_lr,critic_lr, alpha_lr, target_entropy, tau, gamma)\n",
    "    \n",
    "    model = EnsembleDynamicsModel(s_dim, a_dim, model_alpha)\n",
    "    fake_env = FakeEnv(model)\n",
    "    env_pool = ReplayBuffer(MEMORY_CAPACITY)\n",
    "    model_pool = ReplayBuffer(model_pool_size)\n",
    "    rollout_K = variableK(env_pool, model_pool)\n",
    "    mbpo = MBPO(env, sac, fake_env, env_pool, model_pool, rollout_K,\n",
    "                rollout_batch_size, real_ratio, total_episode)\n",
    "\n",
    "    # training part\n",
    "    max_rolling_score = np.float('-inf')\n",
    "    max_score = np.float('-inf')\n",
    "    collision_train = 0\n",
    "    episode_score = np.zeros(total_episode)  # average score of each car following event\n",
    "    rolling_score = np.zeros(total_episode)\n",
    "    cum_collision_num = np.zeros(total_episode)\n",
    "\n",
    "    score_safe = np.zeros(total_episode)\n",
    "    score_efficiency = np.zeros(total_episode)\n",
    "    score_comfort = np.zeros(total_episode)\n",
    "    score_energy = np.zeros(total_episode)\n",
    "    loss_list = np.zeros(total_episode)\n",
    "    K_list = np.zeros(total_episode)\n",
    "    \n",
    "    \n",
    "    # 随机探索采取数据\n",
    "    return_list = []\n",
    "    explore_return = 0\n",
    "    car_fol_id2 = random.randint(0, trainNum - 1)\n",
    "    data2 = train[car_fol_id2, 0]\n",
    "    obs, done, episode_return = env.reset(data2), False, 0\n",
    "    while not done:\n",
    "        action = sac.take_action(obs)[0]\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        env_pool.add(obs, action, reward, next_obs, done)\n",
    "        obs = next_obs\n",
    "        episode_return += reward\n",
    "    return_list.append(explore_return)\n",
    "\n",
    "    for i in tqdm(range(total_episode)):\n",
    "        car_fol_id = random.randint(0, trainNum - 1)\n",
    "        data = train[car_fol_id, 0]\n",
    "        s = env.reset(data)\n",
    "        step = 0\n",
    "        SOC_data = []\n",
    "        SOC = 0.92\n",
    "        SOC_origin = SOC\n",
    "        para = {}\n",
    "        para['k']= 0.5\n",
    "        para['k2'] = 0.5\n",
    "        para['speed'] = s[1]\n",
    "        para['SOC'] = SOC_origin\n",
    "        score = 0\n",
    "        score_s, score_e, score_c, score_eng = 0, 0, 0, 0  # part objective scores\n",
    "\n",
    "        while True:\n",
    "            if step % 20 == 0:\n",
    "                    loss = mbpo.train_model()\n",
    "                    K = mbpo.rollout_model()\n",
    "                    \n",
    "            a = sac.take_action(s)[0]\n",
    "\n",
    "            if CA:\n",
    "                # add collision avoidance guidance\n",
    "                space, svSpd, relSpd = s\n",
    "                lvSpd = svSpd + relSpd\n",
    "                RT = 1  # reaction time\n",
    "                SD = svSpd * RT + (svSpd ** 2 - lvSpd ** 2) / (2 * a_bound)\n",
    "\n",
    "                if space < SD:\n",
    "                    a = - a_bound\n",
    "\n",
    "            para['acc'] = a\n",
    "            SOC_new, cost, INB, out = EV().run(para)\n",
    "            price_elec = cost\n",
    "            r_eng = - 5 * price_elec\n",
    "            \n",
    "            s_, r, done, r_info = env.step(a)\n",
    "            r += r_eng\n",
    "            \n",
    "            env_pool.add(s, a, r, s_, done)\n",
    "            SOC_data.append(SOC_new)\n",
    "\n",
    "            s = s_\n",
    "            \n",
    "            para['speed'] = s[1]\n",
    "            para['SOC'] = SOC_new\n",
    "            \n",
    "            score += r\n",
    "            score_s += r_info[3]\n",
    "            score_e += r_info[4]\n",
    "            score_c += r_info[5]\n",
    "            score_eng += r_eng\n",
    "            \n",
    "            mbpo.update_agent()\n",
    "            step += 1\n",
    "\n",
    "            if done:\n",
    "                duration = data.shape[0]\n",
    "                score /= duration  # normalize with respect to car-following length\n",
    "                score_s /= duration\n",
    "                score_e /= duration\n",
    "                score_c /= duration\n",
    "                score_eng /= duration\n",
    "\n",
    "                if env.isCollision == 1:\n",
    "                    collision_train += 1\n",
    "                break\n",
    "\n",
    "        # record episode results\n",
    "        episode_score[i] = score\n",
    "        score_safe[i] = score_s\n",
    "        score_efficiency[i] = score_e\n",
    "        score_comfort[i] = score_c\n",
    "        score_energy[i] = score_eng\n",
    "        rolling_score[i] = np.mean(episode_score[max(0, i - rolling_window + 1):i + 1])\n",
    "        cum_collision_num[i] = collision_train\n",
    "        loss_list[i] = loss\n",
    "        K_list[i] = K\n",
    "\n",
    "        if max_score < score:\n",
    "            max_score = score\n",
    "\n",
    "        if rolling_score[i] > max_rolling_score:\n",
    "            max_rolling_score = rolling_score[i]\n",
    "            # save network parameters\n",
    "#             sac.savenet(f'model_{run}')\n",
    "#             mbpo.save(i)\n",
    "        if i > total_episode-10:\n",
    "            sac.save(i)\n",
    "            \n",
    "        sys.stdout.write(\n",
    "            f'''\\r Run {run}, Episode {i}, Score: {score:.2f}, Rolling score: {rolling_score[i]:.2f}, loss: {loss_list[i]:.2f}, Max rolling score: {max_rolling_score:.2f}, collisions: {collision_train}   ''')\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    # save results\n",
    "    result.append([episode_score, rolling_score, cum_collision_num, score_safe, score_efficiency, score_comfort,score_energy,loss_list,K_list])\n",
    "\n",
    "np.save(f'result_{run}.npy', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385b94c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rolling_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e25aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episode_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b1e586",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(score_safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78ad8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(score_efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64215e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(score_comfort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9654a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(score_energy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

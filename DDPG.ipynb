{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd68dba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\02Office_software\\anaconda3\\envs\\Torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import scipy.io as sio\n",
    "import pickle as pk\n",
    "import sys\n",
    "import os\n",
    "from simulation_env_new import Env\n",
    "from FourWI_EV_new import EV\n",
    "import rl_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0459527",
   "metadata": {},
   "outputs": [],
   "source": [
    "CA = False\n",
    "total_episode = 500\n",
    "TTC_threshold = 3.001\n",
    "base_name = f'DDPG' \n",
    "\n",
    "actor_lr = 0.001\n",
    "critic_lr = 0.001\n",
    "hidden_dim = 32\n",
    "hidden2_dim = 16\n",
    "gamma = 0.9\n",
    "tau = 0.005  # 软更新参数\n",
    "MEMORY_CAPACITY = 20000\n",
    "# minimal_size = 10000\n",
    "batch_size = 1024\n",
    "sigma = 0.01  # 高斯噪声标准差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8332e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim,hidden2_dim, action_dim, action_bound):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden2_dim)\n",
    "        self.fc3 = torch.nn.Linear(hidden2_dim, action_dim)\n",
    "        self.action_bound = action_bound  # action_bound是环境可以接受的动作最大值\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc2(F.relu(self.fc1(x))))\n",
    "        return torch.tanh(self.fc3(x)) * self.action_bound\n",
    "\n",
    "\n",
    "class QValueNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(QValueNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_out = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        cat = torch.cat([x, a], dim=1) # 拼接状态和动作\n",
    "        x = F.relu(self.fc1(cat))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "778b7964",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    ''' DDPG算法 '''\n",
    "    def __init__(self, state_dim, hidden_dim,hidden2_dim, action_dim, action_bound, sigma, actor_lr, critic_lr, tau, gamma, device):\n",
    "        self.actor = PolicyNet(state_dim, hidden_dim,hidden2_dim, action_dim, action_bound).to(device)\n",
    "        self.critic = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.target_actor = PolicyNet(state_dim, hidden_dim,hidden2_dim, action_dim, action_bound).to(device)\n",
    "        self.target_critic = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        # 初始化目标价值网络并设置和价值网络相同的参数\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        # 初始化目标策略网络并设置和策略相同的参数\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        self.gamma = gamma\n",
    "        self.sigma = sigma  # 高斯噪声的标准差,均值直接设为0\n",
    "        self.tau = tau  # 目标网络软更新参数\n",
    "        self.action_dim = action_dim\n",
    "        self.device = device\n",
    "        self.pointer = 0\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, s_dim * 2 + a_dim + 1), dtype=np.float32)\n",
    "\n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
    "        action = self.actor(state).item()\n",
    "        return action\n",
    "\n",
    "    def soft_update(self, net, target_net):\n",
    "        for param_target, param in zip(target_net.parameters(), net.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - self.tau) + param.data * self.tau)\n",
    "            \n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, a, [r], s_))\n",
    "        index = self.pointer % MEMORY_CAPACITY  # replace the old memory with new memory\n",
    "        self.memory[index, :] = transition\n",
    "        self.pointer += 1\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'], dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'], dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "\n",
    "        next_q_values = self.target_critic(next_states, self.target_actor(next_states))\n",
    "        q_targets = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "        critic_loss = torch.mean(F.mse_loss(self.critic(states, actions), q_targets))\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        actor_loss = -torch.mean(self.critic(states, self.actor(states)))\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.soft_update(self.actor, self.target_actor)  # 软更新策略网络\n",
    "        self.soft_update(self.critic, self.target_critic)  # 软更新价值网络\n",
    "        \n",
    "    def save(self,episode):\n",
    "        torch.save(self.actor.state_dict(), \"./model/DDPG/ddpg_s7_actor{}.pth\".format(episode))\n",
    "#         torch.save(self.actor.state_dict(), \"./model/ddpg_actor.pth\")\n",
    "\n",
    "    def load(self,episode):\n",
    "        self.actor.load_state_dict(torch.load(\"./model/ddpg_actor{}.pth\".format(episode)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9056b32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10678\\AppData\\Local\\Temp\\ipykernel_145268\\902288668.py:36: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_rolling_score = np.float('-inf')\n",
      "C:\\Users\\10678\\AppData\\Local\\Temp\\ipykernel_145268\\902288668.py:37: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_score = np.float('-inf')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 1073\n",
      "Number of validate samples: 268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]C:\\Users\\10678\\AppData\\Local\\Temp\\ipykernel_145268\\261111748.py:23: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  state = torch.tensor([state], dtype=torch.float).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " Run DDPG_Energy_s14, Episode 0, Score: -0.09, Rolling score: -0.09, Max score: -0.09, Max rolling score: -0.09, collisions: 0   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/500 [00:00<05:21,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " Run DDPG_Energy_s14, Episode 1, Score: -0.03, Rolling score: -0.06, Max score: -0.03, Max rolling score: -0.06, collisions: 0   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 2/500 [00:01<06:13,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " Run DDPG_Energy_s14, Episode 2, Score: -0.07, Rolling score: -0.06, Max score: -0.03, Max rolling score: -0.06, collisions: 0   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 3/500 [00:02<06:32,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " Run DDPG_Energy_s14, Episode 3, Score: -0.31, Rolling score: -0.12, Max score: -0.03, Max rolling score: -0.06, collisions: 1   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 4/500 [00:02<05:15,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " Run DDPG_Energy_s14, Episode 4, Score: -0.09, Rolling score: -0.12, Max score: -0.03, Max rolling score: -0.06, collisions: 1   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 5/500 [00:03<05:10,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " Run DDPG_Energy_s14, Episode 5, Score: -0.06, Rolling score: -0.11, Max score: -0.03, Max rolling score: -0.06, collisions: 1   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 6/500 [00:04<06:15,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " Run DDPG_Energy_s14, Episode 6, Score: -0.58, Rolling score: -0.18, Max score: -0.03, Max rolling score: -0.06, collisions: 2   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▏         | 7/500 [00:04<05:12,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " Run DDPG_Energy_s14, Episode 7, Score: -0.65, Rolling score: -0.23, Max score: -0.03, Max rolling score: -0.06, collisions: 3   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 8/500 [00:05<04:40,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " Run DDPG_Energy_s14, Episode 8, Score: -0.09, Rolling score: -0.22, Max score: -0.03, Max rolling score: -0.06, collisions: 3   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 9/500 [00:06<06:25,  1.27it/s]"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "env = Env(TTC_threshold)\n",
    "\n",
    "# load training data\n",
    "# train = sio.loadmat('trainSet.mat')['calibrationData']\n",
    "# test = sio.loadmat('testSet.mat')['validationData']\n",
    "train = sio.loadmat('calibrationData_new.mat')['calibrationData_new']\n",
    "test = sio.loadmat('validationData_new.mat')['validationData_new']\n",
    "trainNum = train.shape[0]\n",
    "testNum = test.shape[0]\n",
    "print('Number of training samples:', trainNum)\n",
    "print('Number of validate samples:', testNum)\n",
    "\n",
    "random.seed(14)\n",
    "np.random.seed(14)\n",
    "torch.manual_seed(14)\n",
    "\n",
    "s_dim = env.n_features\n",
    "a_dim = env.n_actions\n",
    "a_bound = env.action_Bound\n",
    "\n",
    "# # Stop distance collision avoidance\n",
    "n_run = 3\n",
    "rolling_window = 10  # 100 car following events, average score\n",
    "result = []\n",
    "\n",
    "\n",
    "for run in [base_name]:\n",
    "    # name is the name of the experiment, CA is whether use collision avoidance\n",
    "    pointer = 0\n",
    "    replay_buffer = rl_utils.ReplayBuffer(MEMORY_CAPACITY)\n",
    "    ddpg = DDPG(s_dim, hidden_dim,hidden2_dim, a_dim, a_bound, sigma, actor_lr, critic_lr, tau, gamma, device)\n",
    "\n",
    "    # training part\n",
    "    max_rolling_score = np.float('-inf')\n",
    "    max_score = np.float('-inf')\n",
    "    collision_train = 0\n",
    "    episode_score = np.zeros(total_episode)  # average score of each car following event\n",
    "    rolling_score = np.zeros(total_episode)\n",
    "    cum_collision_num = np.zeros(total_episode)\n",
    "    var = 3\n",
    "\n",
    "    score_safe = np.zeros(total_episode)\n",
    "    score_efficiency = np.zeros(total_episode)\n",
    "    score_comfort = np.zeros(total_episode)\n",
    "    score_energy = np.zeros(total_episode)\n",
    "\n",
    "\n",
    "    for i in tqdm(range(total_episode)):\n",
    "        car_fol_id = random.randint(0, trainNum - 1)\n",
    "        data = train[car_fol_id, 0]\n",
    "        s = env.reset(data)\n",
    "        SOC_data = []\n",
    "        SOC = 0.92\n",
    "        SOC_origin = SOC\n",
    "        para = {}\n",
    "        para['k']= 0.5\n",
    "        para['k2'] = 0.5\n",
    "        para['speed'] = s[1]\n",
    "        para['SOC'] = SOC_origin\n",
    "        score = 0\n",
    "        score_s, score_e, score_c, score_eng = 0, 0, 0, 0  # part objective scores\n",
    "\n",
    "        while True:\n",
    "            a = ddpg.take_action(s)\n",
    "            a = np.clip(np.random.normal(a, var), -a_bound, a_bound)\n",
    "\n",
    "            if CA:\n",
    "                # add collision avoidance guidance\n",
    "                space, svSpd, relSpd = s\n",
    "                lvSpd = svSpd + relSpd\n",
    "                RT = 1  # reaction time\n",
    "                SD = svSpd * RT + (svSpd ** 2 - lvSpd ** 2) / (2 * a_bound)\n",
    "\n",
    "                if space < SD:\n",
    "                    a = - a_bound\n",
    "\n",
    "            para['acc'] = a\n",
    "            SOC_new, cost, INB, out = EV().run(para)\n",
    "            price_elec = cost\n",
    "            r_eng = - 5 * price_elec\n",
    "            \n",
    "            s_, r, done, r_info = env.step(a)\n",
    "            \n",
    "            r += r_eng\n",
    "            \n",
    "            SOC_data.append(SOC_new)\n",
    "            replay_buffer.add(s, a, r, s_, done)\n",
    "            pointer += 1 \n",
    "\n",
    "            replace = False\n",
    "            if pointer > MEMORY_CAPACITY:\n",
    "                var *= .9995\n",
    "                b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n",
    "                transition_dict = {'states': b_s, 'actions': b_a, 'next_states': b_ns, 'rewards': b_r, 'dones': b_d}\n",
    "                ddpg.update(transition_dict)\n",
    "                \n",
    "            s = s_\n",
    "                    \n",
    "            para['speed'] = s[1]\n",
    "            para['SOC'] = SOC_new\n",
    "            \n",
    "            score += r\n",
    "            score_s += r_info[3]\n",
    "            score_e += r_info[4]\n",
    "            score_c += r_info[5]\n",
    "            score_eng += r_eng\n",
    "                \n",
    "            if done:\n",
    "                duration = data.shape[0]\n",
    "                score /= duration  # normalize with respect to car-following length\n",
    "                score_s /= duration\n",
    "                score_e /= duration\n",
    "                score_c /= duration\n",
    "                score_eng /= duration\n",
    "\n",
    "                if env.isCollision == 1:\n",
    "                    collision_train += 1\n",
    "                break\n",
    "\n",
    "        # record episode results\n",
    "        episode_score[i] = score\n",
    "        score_safe[i] = score_s\n",
    "        score_efficiency[i] = score_e\n",
    "        score_comfort[i] = score_c\n",
    "        score_energy[i] = score_eng\n",
    "        rolling_score[i] = np.mean(episode_score[max(0, i - rolling_window + 1):i + 1])\n",
    "        cum_collision_num[i] = collision_train\n",
    "\n",
    "        if max_score < score:\n",
    "            max_score = score\n",
    "\n",
    "        if rolling_score[i] > max_rolling_score:\n",
    "            max_rolling_score = rolling_score[i]\n",
    "            # save network parameters\n",
    "#             ddpg.save()\n",
    "\n",
    "        if i > total_episode-10:\n",
    "            ddpg.save(i)\n",
    "            \n",
    "        sys.stdout.write(\n",
    "            f'''\\r Run {run}, Episode {i}, Score: {score:.2f}, Rolling score: {rolling_score[i]:.2f}, Max score: {max_score:.2f}, Max rolling score: {max_rolling_score:.2f}, collisions: {collision_train}   ''')\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    # save results\n",
    "    result.append([episode_score, rolling_score, cum_collision_num, score_safe, score_efficiency, score_comfort,score_energy])\n",
    "\n",
    "np.save(f'result_{run}.npy', result)\n",
    "\n",
    "# 保存结果为MAT格式\n",
    "result_dict = {\n",
    "    'episode_score': episode_score,\n",
    "    'rolling_score': rolling_score,\n",
    "    'cum_collision_num': cum_collision_num,\n",
    "    'score_safe': score_safe,\n",
    "    'score_efficiency': score_efficiency,\n",
    "    'score_comfort': score_comfort,\n",
    "    'score_energy': score_energy\n",
    "}\n",
    "\n",
    "# 保存为MAT文件\n",
    "sio.savemat(f'result_{run}.mat', result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543bfd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cum_collision_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed051c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rolling_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a414e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episode_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d252b562",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(score_safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0d5b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(score_efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbf8d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(score_comfort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c0eb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(score_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0b6c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "score_df = pd.DataFrame()\n",
    "score_df['efficiency'] = score_efficiency\n",
    "score_df['safe'] = score_safe\n",
    "score_df['comfort'] = score_comfort\n",
    "\n",
    "# rolling_window = 20\n",
    "# conduct rolling window\n",
    "%matplotlib inline\n",
    "plt.plot(score_df.efficiency.rolling(window=rolling_window).mean(), label = 'Efficiency')\n",
    "plt.plot(score_df.safe.rolling(window=rolling_window).mean(), label = 'Safety')\n",
    "plt.plot(score_df.comfort.rolling(window=rolling_window).mean(), label = 'Comfort')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85bfe68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
